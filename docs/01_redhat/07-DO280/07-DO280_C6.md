# Capitulo 6 - Habilitación de autoservicio de desarrollo

Configurar para un autoservicio seguro para desarrolladores de múltiples equipos sin el personal de operaciones aprovisionen servicios.

## Quotas en Proyectos y Cluster

- Configurar *quotas* de rcudos de computo y recursos de K8S por cantidades en los proyectos y en el clúster

### Límites de Workloads

Un usuario tipo RBAC con acceso al clúster puede cerar *workloads*. Los límites de las granjas de K8S con limitados, incluso si hay autoescalamiento. Es necesario controlar los crecimientos para evitar desboradmientos de costos o malfuncionamiento de *workloads*. Por ello existen los límites, las cuales existe de 2 tipos:

-  Límites de recursos (**limits**): Valor por superior de los recursos que un *workload* va a utilizar.
-  Límittes de solicitudes (**request**): Valor mínimo de *workload*

Los límites también se pueden aplicar a los *namespaces* para controlar a los *workloads* en la utilización de los recursos con otros *namespaces*

### Quotas de recursos

Se aplica el concepto a los *namespaces* como un tipo de recurso `ResourceQuota`, lo que impide la creación de recursos si el límite es superado, ejemplo:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: memory
  namespace: example
spec:
  hard:
    limits.memory: 4Gi
    requests.memory: 2Gi
  scopes: {}
  scopeSelector: {}
```

#### Quotas de recursos de cómputo

Son 4 definiciones: `limits.cpu`, `limits.memory`, `requests.cpu` y `requests.memory`. Los `limit.*` se aplican a los recursos con el valor máximo a consumir y los `requests.*` a las solicitudes y van enfocadas al máximo de reserva en un *namespace*.

Es contraproducente definir quotas excesivas debido a la suubutilización de recursos

#### Quotas de cantidad de objetos

Por ejemplo el número máximo de *workloads* que puede crearse en un *namespace*. Es importante tener los recursos necesarios para soportar la cantidad de objetos que puede tener un clúster de K8S, ya que de no tenerlo y existir un mal uso de ello puede ocasionar malfuncionamiento o dañar el performance del clúster. 

Es posible que la cantidad de ciertos recursos afectes sistemas externos como un storage, ya que por ejemplo tener volúmenes persistentes y no tener control de su creanción en el Clúster, podría crear problemas.

Con el comando `oc api-resources` con el subargumento en campo vacio `--api-group=""` para listar los grupos del *core*:

```bash
oc api-resources --api-group=""  --namespaced=true
```

#### Creando Quotas a proyectos

Por la parte Web **Administration** → **ResourceQuotas** para crear las *Quotas* aunque también se puede hacer con un editor YAML, con la opción `oc create resourcequota --help` se puede ver ejemplos de creación.

Ejemplo de numero de pods en un *namespace* y posteriormente su equivalente en YAML

```bash
oc create resourcequota example --hard=count/pods=1
```
YAML:
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: example
spec:
  hard:
    count/pods: "1"
```

Para consultar, se puede así:

```bash
oc get quota example -o yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  creationTimestamp: "2024-01-30T17:59:52Z"
  name: example
  namespace: default
  resourceVersion: "193658"
  uid: df12b484-4e78-4920-acb4-e04ab286a4a1
spec:
  hard:
    count/pods: "1"
status:
  hard:
    count/pods: "1"
  used:
    count/pods: "0"

oc get quota
```

Las *quotas* generan métricas las cuales se pueden examinar por la parte Web.

### Troubleshooting de Quotas

Una granja de K8S no puede verificar si una *quota* es correcta o no, por ello se recomienda crearla primero en un ambiente previo y con un valor "supuesto", luego revisar el estaod de *quota*

Validación:

```bash
oc get resourcequota
```

Si un despliegue al momento de ejeuctarse supera una *quota* generará error inmediatamente. Sin menbargo no todas las *quotas* se pueden ver si se superan, por ello es importante ver los eventos de un *namespace*

```bash
oc get event --sort-by .metadata.creationTimestamp
```

Por lla misma ruta por donde se crea una *quota* por Web, por allí también se puede validar el estado.

#### Craando Quotas para varios proyectos

Cuando un equipo de desarrolladores tiene a cargo múltiples *namespaces* puede ser contraproducente asignarle cuotas cada uno de ellos, por ello, OCP puede aplicar *quotas* a nivel del *Clúster* basado en `selectors`, así:

```yaml
apiVersion: quota.openshift.io/v1
kind: ClusterResourceQuota
metadata:
  name: example
spec:
  quota:
    hard:
      limits.cpu: 4
  selector:
    annotations: {}
    labels:
      matchLabels:
        kubernetes.io/metadata.name: example
```

Por el entorno Web se puede llegar a través de **Administration** → **CustomResourceDefinitions** o también utilizando el comando, así:

```bash
oc create clusterresourcequota example --project-label-selector=group=dev --hard=requests.cpu=10
```

Solo los Admin del proyecto pueden validar  us uso de los *Quotas* en un *namepace* específico. El comando no funciona a todos *namespaces* solo al 1 en específico o selecionado

```bash
oc describe AppliedClusterResourceQuota -n example-2
```
### Documentación:

- [Building Applications](https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/building_applications/index#quotas-setting-per-project)
- [Planning Your Environment According to Object Maximums](https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/scalability_and_performance/index#ibm-z-platform)
- [request & Limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)
- [Resources Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/)

## Límites de Recursos por proyecto (Limit Range)

- Configuración default y reuquerimientos de recursos máximo de computo para pods por proyecto

### Gestionando recursos Namespace

A un *namespace* se le puede aplicar quotas para controlar los *workloads que alli utilizan los recursos, y se convierte en una gestión de recursos, para evitar la sobrecarga de *workloads* que impidan la ejecución de otros *workloads*. K8S tiene la capacidad de rangos de límites para los *workloads*

### Límite de rangos

Se aplican a pods, contenedores, imagenes, streams y reclamación de volúmenes persistentes

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: default
spec:
  limits:
    - default:
        memory: 512Mi
      defaultRequest:
        memory: 256Mi
      type: Container
```

Tiene los siguientes tipos:

| Tipo | Descripción |
|:-----|:------------|
| Default limit | La llave en `default` especifica el limite por defecto para los *workloads* |
| Default request | La llave `defaultRequest` especifica el valor por defecto de los *request* para los *workloads* |
| Maximum | La llave `max` especifica el valor máximo tanto de *request* como de *limits* |
| Minimum | La llave `min` especifica el valor mínimo tanto de *request* como de *limits* |
| Limit-to-request ratio | la llave `maxLimitRequestRatio` controla la relación entre *limits* y *request*

#### Configurar rangos de límites máximos y mínimos

- No se pueden crear *workloads* que superen los topes maximos.
- Se utiliza para evitar el alto uso de *request* o *limits*
- *limits* muy bajos puedes impedir que creen *workloads* legitimas
- Útiles para garantizar que los *workloads* cre creen con buena cantidad de *request* y *limits*

#### Valores por defecto

- Son adecuados en los *namespaces* con *quotas*, se evita la configuración de *limits* en cada *workload*
- Si se establece `default` y `defaultRequest` los *workloads* utilizan el límite de rango por defecto
- Ideal para creación dinámica de *workloads*, escenarios de prueba, entre otros
- No adecuado al momento de determinar los valores si el *namespace* tiene *workloads* variados

### Creando rango de límites

*Cuota* del *namespace*:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: example
  namespace: example
spec:
  hard:
    limits.cpu: "8"
    limits.memory: 8Gi
    requests.cpu: "4"
    requests.memory: 4Gi
```

Comando de creación de un *deployment*:

```bash
oc create deployment example --image=image
```

Mensaje de *quota* evitando la creación de pods:

```bash
oc get event --sort-by .metadata.creationTimestamp
LAST SEEN   TYPE      REASON              OBJECT                          MESSAGE
...output omitted...
13s         Warning   FailedCreate        replicaset/example-74c57c8dff   Error creating: pods "example-74c57c8dff-rzl7w" is forbidden: failed quota: example: must specify limits.cpu for: hello-world-nginx; limits.memory for: hello-world-nginx; requests.cpu for: hello-world-nginx; requests.memory for: hello-world-nginx
```

Ejemplo de *limite de rango* de todod los tipos:

```yaml
piVersion: v1
kind: LimitRange
metadata:
  name: example
  namespace: example
spec:
  limits:
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 250m
      memory: 256Mi
    max:
      cpu: "1"
      memory: 1Gi
    min:
      cpu: 125m
      memory: 128Mi
    type: Container
```
Los límites de rango no afectan los pods existentes. Si se elimina el *deployment* y luego lo crea con `oc create`, el *deployment* crea los pods aplicando el límite de rango:

```bash
oc describe pod
.
.
.
Containers:
  hello-world-nginx:
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:        250m
      memory:     256Mi
.
.
.
```

Modificación directa de límite en un *deployment*

```bash
oc set resources deployment example --limits=cpu=new-cpu-limit
```

Validación:

```bash
oc get event --sort-by .metadata.creationTimestamp
```

NOTA: validar al momento de un nuevo despliegue, ya que las *quote* pueden afectar el *rollout*

- El valor max de CPU/MEM debe ser mayor o igual que el valor por defecto.
- El valor por defecto de CPU/MEM debe ser mayor o igual que el valor defaultRequest.
- El valor defaultRequest de CPU/MEM debe ser mayor o igual que el valor min.
- No aplicar valores que esten en conflicto

### Documentación

- [Restrict Resource Consumption with Limit Ranges ](https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/nodes/index#nodes-cluster-limit-ranges)

## Template de Proyecto y Role de Autoaprovisionamiento
