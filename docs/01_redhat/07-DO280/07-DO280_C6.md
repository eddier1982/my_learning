# Capitulo 6 - Habilitación de autoservicio de desarrollo

Configurar para un autoservicio seguro para desarrolladores de múltiples equipos sin el personal de operaciones aprovisionen servicios.

## Quotas en Proyectos y Cluster

- Configurar *quotas* de rcudos de computo y recursos de K8S por cantidades en los proyectos y en el clúster

### Límites de Workloads

Un usuario tipo RBAC con acceso al clúster puede cerar *workloads*. Los límites de las granjas de K8S con limitados, incluso si hay autoescalamiento. Es necesario controlar los crecimientos para evitar desboradmientos de costos o malfuncionamiento de *workloads*. Por ello existen los límites, las cuales existe de 2 tipos:

-  Límites de recursos (**limits**): Valor por superior de los recursos que un *workload* va a utilizar.
-  Límittes de solicitudes (**request**): Valor mínimo de *workload*

Los límites también se pueden aplicar a los *namespaces* para controlar a los *workloads* en la utilización de los recursos con otros *namespaces*

### Quotas de recursos

Se aplica el concepto a los *namespaces* como un tipo de recurso `ResourceQuota`, lo que impide la creación de recursos si el límite es superado, ejemplo:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: memory
  namespace: example
spec:
  hard:
    limits.memory: 4Gi
    requests.memory: 2Gi
  scopes: {}
  scopeSelector: {}
```

#### Quotas de recursos de cómputo

Son 4 definiciones: `limits.cpu`, `limits.memory`, `requests.cpu` y `requests.memory`. Los `limit.*` se aplican a los recursos con el valor máximo a consumir y los `requests.*` a las solicitudes y van enfocadas al máximo de reserva en un *namespace*.

Es contraproducente definir quotas excesivas debido a la suubutilización de recursos

#### Quotas de cantidad de objetos

Por ejemplo el número máximo de *workloads* que puede crearse en un *namespace*. Es importante tener los recursos necesarios para soportar la cantidad de objetos que puede tener un clúster de K8S, ya que de no tenerlo y existir un mal uso de ello puede ocasionar malfuncionamiento o dañar el performance del clúster. 

Es posible que la cantidad de ciertos recursos afectes sistemas externos como un storage, ya que por ejemplo tener volúmenes persistentes y no tener control de su creanción en el Clúster, podría crear problemas.

Con el comando `oc api-resources` con el subargumento en campo vacio `--api-group=""` para listar los grupos del *core*:

```bash
oc api-resources --api-group=""  --namespaced=true
```

#### Creando Quotas a proyectos

Por la parte Web **Administration** → **ResourceQuotas** para crear las *Quotas* aunque también se puede hacer con un editor YAML, con la opción `oc create resourcequota --help` se puede ver ejemplos de creación.

Ejemplo de numero de pods en un *namespace* y posteriormente su equivalente en YAML

```bash
oc create resourcequota example --hard=count/pods=1
```
YAML:
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: example
spec:
  hard:
    count/pods: "1"
```

Para consultar, se puede así:

```bash
oc get quota example -o yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  creationTimestamp: "2024-01-30T17:59:52Z"
  name: example
  namespace: default
  resourceVersion: "193658"
  uid: df12b484-4e78-4920-acb4-e04ab286a4a1
spec:
  hard:
    count/pods: "1"
status:
  hard:
    count/pods: "1"
  used:
    count/pods: "0"

oc get quota
```

Las *quotas* generan métricas las cuales se pueden examinar por la parte Web.

### Troubleshooting de Quotas

Una granja de K8S no puede verificar si una *quota* es correcta o no, por ello se recomienda crearla primero en un ambiente previo y con un valor "supuesto", luego revisar el estaod de *quota*

Validación:

```bash
oc get resourcequota
```

Si un despliegue al momento de ejeuctarse supera una *quota* generará error inmediatamente. Sin menbargo no todas las *quotas* se pueden ver si se superan, por ello es importante ver los eventos de un *namespace*

```bash
oc get event --sort-by .metadata.creationTimestamp
```

Por lla misma ruta por donde se crea una *quota* por Web, por allí también se puede validar el estado.

#### Craando Quotas para varios proyectos

Cuando un equipo de desarrolladores tiene a cargo múltiples *namespaces* puede ser contraproducente asignarle cuotas cada uno de ellos, por ello, OCP puede aplicar *quotas* a nivel del *Clúster* basado en `selectors`, así:

```yaml
apiVersion: quota.openshift.io/v1
kind: ClusterResourceQuota
metadata:
  name: example
spec:
  quota:
    hard:
      limits.cpu: 4
  selector:
    annotations: {}
    labels:
      matchLabels:
        kubernetes.io/metadata.name: example
```

Por el entorno Web se puede llegar a través de **Administration** → **CustomResourceDefinitions** o también utilizando el comando, así:

```bash
oc create clusterresourcequota example --project-label-selector=group=dev --hard=requests.cpu=10
```

Solo los Admin del proyecto pueden validar  us uso de los *Quotas* en un *namepace* específico. El comando no funciona a todos *namespaces* solo al 1 en específico o selecionado

```bash
oc describe AppliedClusterResourceQuota -n example-2
```
### Documentación:

- [Building Applications](https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/building_applications/index#quotas-setting-per-project)
- [Planning Your Environment According to Object Maximums](https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/scalability_and_performance/index#ibm-z-platform)
- [request & Limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)
- [Resources Quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/)

## Límites de Recursos por proyecto (Limit Range)

- Configuración default y reuquerimientos de recursos máximo de computo para pods por proyecto

### Gestionando recursos Namespace

A un *namespace* se le puede aplicar quotas para controlar los *workloads que alli utilizan los recursos, y se convierte en una gestión de recursos, para evitar la sobrecarga de *workloads* que impidan la ejecución de otros *workloads*. K8S tiene la capacidad de rangos de límites para los *workloads*

### Límite de rangos

Se aplican a pods, contenedores, imagenes, streams y reclamación de volúmenes persistentes

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
  namespace: default
spec:
  limits:
    - default:
        memory: 512Mi
      defaultRequest:
        memory: 256Mi
      type: Container
```

Tiene los siguientes tipos:

| Tipo | Descripción |
|:-----|:------------|
| Default limit | La llave en `default` especifica el limite por defecto para los *workloads* |
| Default request | La llave `defaultRequest` especifica el valor por defecto de los *request* para los *workloads* |
| Maximum | La llave `max` especifica el valor máximo tanto de *request* como de *limits* |
| Minimum | La llave `min` especifica el valor mínimo tanto de *request* como de *limits* |
| Limit-to-request ratio | la llave `maxLimitRequestRatio` controla la relación entre *limits* y *request*

#### Configurar rangos de límites máximos y mínimos

- No se pueden crear *workloads* que superen los topes maximos.
- Se utiliza para evitar el alto uso de *request* o *limits*
- *limits* muy bajos puedes impedir que creen *workloads* legitimas
- Útiles para garantizar que los *workloads* cre creen con buena cantidad de *request* y *limits*

#### Valores por defecto

- Son adecuados en los *namespaces* con *quotas*, se evita la configuración de *limits* en cada *workload*
- Si se establece `default` y `defaultRequest` los *workloads* utilizan el límite de rango por defecto
- Ideal para creación dinámica de *workloads*, escenarios de prueba, entre otros
- No adecuado al momento de determinar los valores si el *namespace* tiene *workloads* variados

### Creando rango de límites

*Cuota* del *namespace*:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: example
  namespace: example
spec:
  hard:
    limits.cpu: "8"
    limits.memory: 8Gi
    requests.cpu: "4"
    requests.memory: 4Gi
```

Comando de creación de un *deployment*:

```bash
oc create deployment example --image=image
```

Mensaje de *quota* evitando la creación de pods:

```bash
oc get event --sort-by .metadata.creationTimestamp
LAST SEEN   TYPE      REASON              OBJECT                          MESSAGE
...output omitted...
13s         Warning   FailedCreate        replicaset/example-74c57c8dff   Error creating: pods "example-74c57c8dff-rzl7w" is forbidden: failed quota: example: must specify limits.cpu for: hello-world-nginx; limits.memory for: hello-world-nginx; requests.cpu for: hello-world-nginx; requests.memory for: hello-world-nginx
```

Ejemplo de *limite de rango* de todod los tipos:

```yaml
piVersion: v1
kind: LimitRange
metadata:
  name: example
  namespace: example
spec:
  limits:
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 250m
      memory: 256Mi
    max:
      cpu: "1"
      memory: 1Gi
    min:
      cpu: 125m
      memory: 128Mi
    type: Container
```
Los límites de rango no afectan los pods existentes. Si se elimina el *deployment* y luego lo crea con `oc create`, el *deployment* crea los pods aplicando el límite de rango:

```bash
oc describe pod
.
.
.
Containers:
  hello-world-nginx:
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:        250m
      memory:     256Mi
.
.
.
```

Modificación directa de límite en un *deployment*

```bash
oc set resources deployment example --limits=cpu=new-cpu-limit
```

Validación:

```bash
oc get event --sort-by .metadata.creationTimestamp
```

NOTA: validar al momento de un nuevo despliegue, ya que las *quote* pueden afectar el *rollout*

- El valor max de CPU/MEM debe ser mayor o igual que el valor por defecto.
- El valor por defecto de CPU/MEM debe ser mayor o igual que el valor defaultRequest.
- El valor defaultRequest de CPU/MEM debe ser mayor o igual que el valor min.
- No aplicar valores que esten en conflicto

### Documentación

- [Restrict Resource Consumption with Limit Ranges ](https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/nodes/index#nodes-cluster-limit-ranges)

## Template de Proyecto y Role de Autoaprovisionamiento

- Configurar quotas default, limit range, role bindings y otras restricciones para nuevos proyectos, y permitir a usuarios el autoaprovisionamiento de proyectos.

### Creación de Proyecto

K8S provee *namespaces* para separar *workloads* y su metadata tiene implicaciones de seguridad en un clúster, puede existir *labels* a nivel de los controladores de políticas que limitan las capacidades del *ns*. Si los usuarios puede modificar el *ns* también pueden llegar a modificar la metadata del *ns* y sobreescribir las opciones de seguridad.

OCP introduce Proyectos para mejorar la seguridad y la experiencia de trabajo con *ns*. Se tiene el tipo de recurso `Project` que está en el API de OCP. Con esta opción se puede listar los *ns*, filtrarlos por los visible para el usuario y regresar en formato visible del proyecto.

Tambien se incorpora en OCP `Projectrequest` como otro tipo de recurso. Este permite al momento de realizar una solicitud al proyecto, el API de OCP puede crear un *ns* desde un template (el template puede contener por defecto permisos, quoatas o rangos de límites).

Todas estas características son parte del autoservicio de gestión de *ns* y tener la acapacidad de entregarle a un usuario el role de creación de *ns* sin la opción de modificar su metadata.

### Planeando un Project Template

Puede agregar los siguientes tipos de recursos a un *Project Template*

| Tipos de Project Template | Descripción |
|:--------------------------|:------------|
| Roles & role bindings | Por default el template entrega el role de Admin a quien solicita el proyecto, este permiso se puede mantener o utlizar otro similar. Se pueden agregar otros tipos de permisos. | 
| Resource quotas & limit ranges | Con esta opción se garantiza que los nuevos proyectos tengan límites |
| Network policies | Políticas de red para fortalecer la separación de requerimientos de red |

### Creando un Project Template

Se utiliza el comando `oc adm create-bootstrap-project-template` para imprimir el tamplate que se puede usar para la creación de su proyecto template.

Creación inicial del template:

```bash
oc adm create-bootstrap-project-template -o yaml > file
```

El cual tendrá el siguiente contenido:

```yaml
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  creationTimestamp: null
  name: project-request
objects:
- apiVersion: project.openshift.io/v1
  kind: Project
  metadata:
    annotations:
      openshift.io/description: ${PROJECT_DESCRIPTION}
      openshift.io/display-name: ${PROJECT_DISPLAYNAME}
      openshift.io/requester: ${PROJECT_REQUESTING_USER}
    creationTimestamp: null
    name: ${PROJECT_NAME}
  spec: {}
  status: {}
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: admin
    namespace: ${PROJECT_NAME}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: admin
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: ${PROJECT_ADMIN_USER}
parameters:
- name: PROJECT_NAME
- name: PROJECT_DISPLAYNAME
- name: PROJECT_DESCRIPTION
- name: PROJECT_ADMIN_USER
- name: PROJECT_REQUESTING_USER
```

Modifique la lista de objetos y adicione los recursos requeridos para el nuevo *ns*

Con el siguiente comando se puede ver como muestra una lista de objetos en un formato similar a la llave de un template:

```bash
oc get limitrange,resourcequota -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: LimitRange
  metadata:
    creationTimestamp: "2024-01-31T17:48:23Z"
    name: example
    namespace: example
    resourceVersion: "881771"
    uid: d0c19c60-00a9-4028-acc5-22680f1ea658
  spec:
    limits:
    - default:
        cpu: 500m
        memory: 512Mi
      defaultRequest:
        cpu: 250m
        memory: 256Mi
      max:
        cpu: "1"
        memory: 1Gi
      min:
        cpu: 125m
        memory: 128Mi
      type: Container
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    creationTimestamp: "2024-01-31T17:48:04Z"
    name: example
    namespace: example
    resourceVersion: "881648"
    uid: 108f0771-dc11-4289-ae76-6514d58bbece
  spec:
    hard:
      count/pods: "1"
  status:
.
.
.
kind: List
metadata:
  resourceVersion: ""
```

Pasos para la definir un Project Template:
- Crear el *ns*
- Crear los recursos que eligió y realizar pruebas hasta vallidar el comportamiento deseado
- Listar los recursos en el YAML
- Editar el listado de recursos para confirmar que las definiciones ceran los recursos correctos
- Modificar la variable por el valor real
- Adicionar la lista de recursos al template del proyecto que se genera con el comando `oc adm create-bootstrap-project-template` 

***NOTA:*** No se recomienda copiar las definiciones de un recurso existente, ya que puede ocacionar resultados incorrectos o entregar los atributos que no cumplan con lo deseado.

Creación del *template*:

```bash
oc create -f template -n openshift-config
```

### Configurando le Project Template

Para usar un nuevo *template* se modifica el recurso: `projects.config.openshift.io/cluster` en la sección `spec`, por defecto el nombre del *template* es `project-request`

```yaml
apiVersion: config.openshift.io/v1
kind: Project
metadata:
.
.
.
  name: cluster
.
.
.
spec:
  projectRequestTemplate:
    name: project-request
```

Una actualización correcta genera una nueva versión del recurso `apiserver` del *ns* `openshift-apiserver`

***NOTA:*** durante el update del `apiserver` se pueden producir resultados inesperados en las solicitudes al API.

Para restablecer el *template* a su valor original, se modifica el recurso `spec` en formato {} que estpa en `projects.config.openshift.io/cluster`

### Gestión de permisos de autoaprovisionamiento.

Los usuarios con este role pueden crear proyecto y por default la tienen todos los usuarios autenticados. Se puede controlar el *binding* de el role para limitar que usuarios puede hacer solicitud de nuevos proyectos.

Visualizar  el *role binding*

```bash
oc describe clusterrolebinding.rbac self-provisioners
Name:         self-provisioners
Labels:       <none>
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  self-provisioner
Subjects:
  Kind   Name                        Namespace
  ----   ----                        ---------
  Group  system:authenticated:oauth
```

Para realizar cambios, desactive las actualizaciones automáticas con la anotación y edite los temas en la vinculación.

***Importante*** El comando oc adm policy remove-cluster-role-from-group elimina la vinculación del rol de clúster cuando se elimina el último asunto.

Deshabilitar el autoaprovisionamiento:

```bash
oc annotate clusterrolebinding/self-provisioners --overwrite rbac.authorization.kubernetes.io/autoupdate=false
clusterrolebinding.rbac.authorization.k8s.io/self-provisioners annotated

oc patch clusterrolebinding.rbac self-provisioners -p '{"subjects": null}'
clusterrolebinding.rbac.authorization.k8s.io/self-provisioners patched
```

Ejemplo de edición:

```bash
oc edit clusterrolebinding/self-provisioners
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
.
.
.
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: self-provisioner
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: provisioners
```

### Documentación

- [Configuring Project Creation ](https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html-single/building_applications/index#configuring-project-creation)
- [Customizing OpenShift Project Creation](https://developers.redhat.com/blog/2020/02/05/customizing-openshift-project-creation/)